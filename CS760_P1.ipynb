{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS760_P1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **CNN**"
      ],
      "metadata": {
        "id": "OiLeGBLn0gOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "# define model structure\n",
        "def SimpleCNN_Sam():\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(4, (5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu'))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(10))\n",
        "    return model\n",
        "\n",
        "# function to train model on specified training set and test set\n",
        "def train(model, train_data, train_label, test_data, test_label, train_acc, test_acc, epochs):\n",
        "    # define optimizer and loss function to use\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        history = model.fit(train_data, train_label)\n",
        "        train_loss, train_accuracy = model.evaluate(test_data,  test_label, verbose=2)\n",
        "\n",
        "        # append accuracy to lists\n",
        "        train_acc += history.history['accuracy']\n",
        "        test_acc.append(train_accuracy)"
      ],
      "metadata": {
        "id": "aRI7UGgQ0f7U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example Forgetting and Simple Testing**"
      ],
      "metadata": {
        "id": "wVA44OLHp5H8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "hdJRjf0jnNTu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "# define model structure\n",
        "def SimpleCNN():\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(4, (5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu'))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(10))\n",
        "    return model\n",
        "\n",
        "# function to train model on specified training set and test set\n",
        "def train_f(model, train_data, train_label, epochs):\n",
        "    # define optimizer and loss function to use\n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "    \n",
        "    stat = {}\n",
        "    for i in range(epochs):\n",
        "        random_arrange = random.sample(range(len(train_data)), len(train_data))\n",
        "        train = []\n",
        "        label = [] \n",
        "        for i in range(len(train_data)):\n",
        "            train.append(train_data[random_arrange[i]])\n",
        "            label.append(train_label[random_arrange[i]])\n",
        "\n",
        "        model.compile(optimizer=opt,\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "        history = model.fit(tf.convert_to_tensor(train), tf.convert_to_tensor(label))\n",
        "\n",
        "        probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
        "        predictions = probability_model.predict(train_data)\n",
        "        for i in range(len(train_data)):\n",
        "            if i in stat:\n",
        "              stat[i].append(np.argmax(predictions[i]) == train_label[i])\n",
        "            else:\n",
        "              stat[i] = [np.argmax(predictions[i]) == train_label[i]]\n",
        "            \n",
        "    forgetness = {}\n",
        "    \n",
        "    for index, lst in stat.items():\n",
        "      acc_full = np.array(list(map(int, lst)))\n",
        "      transition = acc_full[1:] - acc_full[:-1]\n",
        "      if len(np.where(transition == -1)[0]) > 0:\n",
        "        forgetness[index] = len(np.where(transition == -1)[0])\n",
        "      elif len(np.where(acc_full == 1)[0]) == 0:\n",
        "        forgetness[index] = epochs\n",
        "      else:\n",
        "        forgetness[index] = 0\n",
        "  \n",
        "    #print(dict(sorted(forgetness.items(), key = lambda item: item[1], reverse = True)))\n",
        "    result = []\n",
        "    for i,j in forgetness.items():\n",
        "      if i <= len(train_data) * 0.1:\n",
        "        result.append(i)\n",
        "        \n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test code for Example Forgetting**"
      ],
      "metadata": {
        "id": "p0wduMiKpuQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "model = SimpleCNN()\n",
        "result = train_f(model, train_images, train_labels, 5)\n",
        "len(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD0asL45n5Rm",
        "outputId": "4a7713c9-386d-4889-dcb7-fc737234171d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 22s 11ms/step - loss: 0.5583 - accuracy: 0.8292\n",
            "1875/1875 [==============================] - 22s 12ms/step - loss: 0.2032 - accuracy: 0.9412\n",
            "1875/1875 [==============================] - 22s 11ms/step - loss: 0.1707 - accuracy: 0.9489\n",
            "1875/1875 [==============================] - 22s 12ms/step - loss: 0.1465 - accuracy: 0.9562\n",
            "1875/1875 [==============================] - 22s 12ms/step - loss: 0.1326 - accuracy: 0.9607\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "649"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Continual Learning 1: Independent Tasks**"
      ],
      "metadata": {
        "id": "xil0ivv-qU4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split training data into 2 different data sets\n",
        "choice = np.random.choice(range(train_images.shape[0]), size=(len(train_images)//2,), replace=False)\n",
        "ind = np.zeros(train_images.shape[0], dtype=bool)\n",
        "ind[choice] = True\n",
        "\n",
        "first_data = train_images[ind]\n",
        "first_label = train_labels[ind]\n",
        "second_data = train_images[~ind]\n",
        "second_label = train_labels[~ind]\n",
        "\n",
        "# split first data into training (80%) and test data set (20%)\n",
        "choice = np.random.choice(range(first_data.shape[0]), size=(int(len(first_data)*0.8),), replace=False)\n",
        "ind = np.zeros(first_data.shape[0], dtype=bool)\n",
        "ind[choice] = True\n",
        "\n",
        "first_data_train = first_data[ind]\n",
        "first_label_train = first_label[ind]\n",
        "first_data_test= first_data[~ind]\n",
        "first_label_test = first_label[~ind]\n",
        "\n",
        "# train on the first dataset and check the test accuracy\n",
        "model = SimpleCNN_Sam()\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "train(model, first_data_train, first_label_train, first_data_test, first_label_test, train_acc, test_acc, 5)\n",
        "model.evaluate(first_data_test, first_label_test, verbose=2)\n",
        "\n",
        "# use this model to predict second data\n",
        "model.evaluate(second_data, second_label, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBifY9oypCcj",
        "outputId": "49c24de0-fea7-41ec-eb91-12a79c43a14f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "750/750 [==============================] - 10s 12ms/step - loss: 2.1960 - accuracy: 0.1717\n",
            "188/188 - 1s - loss: 2.0294 - accuracy: 0.2177 - 1s/epoch - 6ms/step\n",
            "750/750 [==============================] - 9s 12ms/step - loss: 2.0043 - accuracy: 0.2199\n",
            "188/188 - 1s - loss: 1.9632 - accuracy: 0.2332 - 939ms/epoch - 5ms/step\n",
            "750/750 [==============================] - 9s 12ms/step - loss: 1.8009 - accuracy: 0.2932\n",
            "188/188 - 1s - loss: 1.5290 - accuracy: 0.3933 - 937ms/epoch - 5ms/step\n",
            "750/750 [==============================] - 9s 12ms/step - loss: 0.8119 - accuracy: 0.7442\n",
            "188/188 - 1s - loss: 0.5398 - accuracy: 0.8787 - 942ms/epoch - 5ms/step\n",
            "750/750 [==============================] - 9s 12ms/step - loss: 0.3323 - accuracy: 0.9153\n",
            "188/188 - 1s - loss: 0.2439 - accuracy: 0.9303 - 942ms/epoch - 5ms/step\n",
            "188/188 - 1s - loss: 0.2439 - accuracy: 0.9303 - 994ms/epoch - 5ms/step\n",
            "938/938 - 5s - loss: 0.2353 - accuracy: 0.9335 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.23532122373580933, 0.9334666728973389]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split second data into training (80%) and test data set (20%)\n",
        "choice = np.random.choice(range(second_data.shape[0]), size=(int(len(second_data)*0.8),), replace=False)\n",
        "ind = np.zeros(second_data.shape[0], dtype=bool)\n",
        "ind[choice] = True\n",
        "\n",
        "second_data_train = second_data[ind]\n",
        "second_label_train = second_label[ind]\n",
        "second_data_test = second_data[~ind]\n",
        "second_label_test = second_label[~ind]\n",
        "\n",
        "# train on the second dataset and check the test accuracy\n",
        "model = SimpleCNN_Sam()\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "train(model, second_data_train, second_label_train, second_data_test, second_label_test, train_acc, test_acc, 5)\n",
        "model.evaluate(second_data_test, second_label_test, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWzlsnFa2e0-",
        "outputId": "5c4470d0-7ee1-4fdd-a02c-6ff6304b8dad"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "750/750 [==============================] - 9s 12ms/step - loss: 2.4071 - accuracy: 0.2241\n",
            "188/188 - 1s - loss: 1.6373 - accuracy: 0.3683 - 1s/epoch - 6ms/step\n",
            "750/750 [==============================] - 9s 12ms/step - loss: 1.3376 - accuracy: 0.4941\n",
            "188/188 - 1s - loss: 1.1750 - accuracy: 0.5700 - 929ms/epoch - 5ms/step\n",
            "750/750 [==============================] - 9s 12ms/step - loss: 1.0730 - accuracy: 0.5989\n",
            "188/188 - 1s - loss: 1.0096 - accuracy: 0.6148 - 920ms/epoch - 5ms/step\n",
            "750/750 [==============================] - 9s 12ms/step - loss: 0.9147 - accuracy: 0.6467\n",
            "188/188 - 1s - loss: 0.8338 - accuracy: 0.6705 - 912ms/epoch - 5ms/step\n",
            "750/750 [==============================] - 9s 12ms/step - loss: 0.6816 - accuracy: 0.7588\n",
            "188/188 - 1s - loss: 0.6067 - accuracy: 0.8093 - 928ms/epoch - 5ms/step\n",
            "188/188 - 1s - loss: 0.6067 - accuracy: 0.8093 - 928ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6066831350326538, 0.809333324432373]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using first model to predict the second data set, accuracy is 93%. However, using second model to predict the second data set, accuracy is 81%. There is some forgetting."
      ],
      "metadata": {
        "id": "JPcFjmo13Bw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Continual Learning 2: Random Selection Replay Strategy**"
      ],
      "metadata": {
        "id": "gefi-3vr3RKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import array\n",
        "# randomly chosen 10% (hyperparameter?) of data from first data\n",
        "choice = np.random.choice(range(train_images.shape[0]), size=(int(len(second_data)*0.1),), replace=False)\n",
        "ind = np.zeros(train_images.shape[0], dtype=bool)\n",
        "ind[choice] = True\n",
        "\n",
        "data_to_add = train_images[ind]\n",
        "label_to_add = train_labels[ind]\n",
        "\n",
        "second_data_random = []\n",
        "second_label_random = []\n",
        "for element in second_data_train:\n",
        "  second_data_random.append(element)\n",
        "for element in data_to_add:\n",
        "  second_data_random.append(element)\n",
        "for element in second_label_train:\n",
        "  second_label_random.append(element)\n",
        "for element in label_to_add:\n",
        "  second_label_random.append(element)\n",
        "\n",
        "second_data_random = np.array(second_data_random)\n",
        "second_label_random = np.array(second_label_random)\n",
        "\n",
        "# train on the second dataset and check the test accuracy\n",
        "model = SimpleCNN_Sam()\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "train(model, second_data_random, second_label_random, second_data_test, second_label_test, train_acc, test_acc, 5)\n",
        "model.evaluate(second_data_test, second_label_test, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vekIrN553aRW",
        "outputId": "f427ecc7-e45c-42cc-878e-6518f3c98b1e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "844/844 [==============================] - 10s 12ms/step - loss: 2.3390 - accuracy: 0.2019\n",
            "188/188 - 1s - loss: 2.0441 - accuracy: 0.2102 - 1s/epoch - 5ms/step\n",
            "844/844 [==============================] - 10s 12ms/step - loss: 1.9501 - accuracy: 0.2425\n",
            "188/188 - 1s - loss: 1.7990 - accuracy: 0.3098 - 913ms/epoch - 5ms/step\n",
            "844/844 [==============================] - 10s 12ms/step - loss: 1.5503 - accuracy: 0.4111\n",
            "188/188 - 1s - loss: 1.1949 - accuracy: 0.5903 - 909ms/epoch - 5ms/step\n",
            "844/844 [==============================] - 10s 12ms/step - loss: 0.8360 - accuracy: 0.7263\n",
            "188/188 - 1s - loss: 0.6285 - accuracy: 0.7857 - 926ms/epoch - 5ms/step\n",
            "844/844 [==============================] - 10s 12ms/step - loss: 0.4685 - accuracy: 0.8759\n",
            "188/188 - 1s - loss: 0.4101 - accuracy: 0.9007 - 913ms/epoch - 5ms/step\n",
            "188/188 - 1s - loss: 0.4101 - accuracy: 0.9007 - 909ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4100704789161682, 0.9006666541099548]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With random selection of data from the first data and put into the second test data, the test accuracy improved to 90%."
      ],
      "metadata": {
        "id": "NXPAvwOb9iXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Continual Learning 3: Example Forgetting Replay Strategy**"
      ],
      "metadata": {
        "id": "cYbSSke49xCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain data with highest forgetting statistics from the first data\n",
        "model = SimpleCNN()\n",
        "result = train_f(model, first_data, first_label, 5)\n",
        "print(len(result)/len(first_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hEO30h79wav",
        "outputId": "a45b1597-0a91-48bc-80ad-847ccb7b03f9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "938/938 [==============================] - 11s 12ms/step - loss: 1.5536 - accuracy: 0.5051\n",
            "938/938 [==============================] - 11s 12ms/step - loss: 0.7395 - accuracy: 0.7897\n",
            "938/938 [==============================] - 11s 12ms/step - loss: 0.5150 - accuracy: 0.8579\n",
            "938/938 [==============================] - 11s 12ms/step - loss: 0.4007 - accuracy: 0.8904\n",
            "938/938 [==============================] - 11s 12ms/step - loss: 0.3403 - accuracy: 0.9086\n",
            "0.10003333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10% of data is included into second data test set"
      ],
      "metadata": {
        "id": "a7OTNKB5_CBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "n9VLCiIC_Wz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_data_forget = []\n",
        "second_label_forget = []\n",
        "for element in second_data_train:\n",
        "  second_data_forget.append(element)\n",
        "for element in result:\n",
        "  second_data_forget.append(first_data[element])\n",
        "for element in second_label_train:\n",
        "  second_label_forget.append(element)\n",
        "for element in result:\n",
        "  second_label_forget.append(first_label[element])\n",
        "\n",
        "second_data_forget = np.array(second_data_forget)\n",
        "second_label_forget = np.array(second_label_forget)\n",
        "\n",
        "# train on the second dataset and check the test accuracy\n",
        "model = SimpleCNN_Sam()\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "train(model, second_data_forget, second_label_forget, second_data_test, second_label_test, train_acc, test_acc, 5)\n",
        "model.evaluate(second_data_test, second_label_test, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeMLxo2T_IZO",
        "outputId": "7b11814f-fc66-4d62-c301-49a828117fbb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "844/844 [==============================] - 10s 12ms/step - loss: 2.1202 - accuracy: 0.2476\n",
            "188/188 - 1s - loss: 1.7917 - accuracy: 0.2975 - 1s/epoch - 5ms/step\n",
            "844/844 [==============================] - 10s 12ms/step - loss: 1.6349 - accuracy: 0.3628\n",
            "188/188 - 1s - loss: 1.3750 - accuracy: 0.4572 - 938ms/epoch - 5ms/step\n",
            "844/844 [==============================] - 10s 12ms/step - loss: 1.1895 - accuracy: 0.5335\n",
            "188/188 - 1s - loss: 1.0074 - accuracy: 0.6073 - 912ms/epoch - 5ms/step\n",
            "844/844 [==============================] - 10s 12ms/step - loss: 0.6574 - accuracy: 0.7992\n",
            "188/188 - 1s - loss: 0.3727 - accuracy: 0.9013 - 913ms/epoch - 5ms/step\n",
            "844/844 [==============================] - 10s 12ms/step - loss: 0.2754 - accuracy: 0.9236\n",
            "188/188 - 1s - loss: 0.2549 - accuracy: 0.9353 - 909ms/epoch - 5ms/step\n",
            "188/188 - 1s - loss: 0.2549 - accuracy: 0.9353 - 919ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2549079358577728, 0.9353333115577698]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, the accuracy improved to 93.5%!"
      ],
      "metadata": {
        "id": "22h3awY5CDRt"
      }
    }
  ]
}